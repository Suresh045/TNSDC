# -*- coding: utf-8 -*-
"""3_DC_Grid_less_parameters CLASSIFICATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SC5vj0GkU38J3kb39qV0VQiqjtu9Ny4-
"""

#importing the Libraies
import numpy as np
import pandas as pd

# Reading the Dataset
dataset = pd.read_csv('/content/Social_Network_Ads.csv')

dataset.head()

dataset.tail()

dataset=pd.get_dummies(dataset,dtype=int,drop_first=True)

dataset

X=dataset[['Age', 'EstimatedSalary','Gender_Male']]
y=dataset['Purchased']

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)

from sklearn.svm import SVC

#https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter

from sklearn.model_selection import GridSearchCV, cross_val_predict

# Define parameter grid (smaller for speed)
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto'],
    'class_weight': [None, 'balanced']
}

grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,n_jobs=-1,scoring='f1_weighted')
# fitting the model for grid search
grid.fit(X,y)

# GridSearchCV with F1 score
from sklearn.tree import DecisionTreeClassifier
# ✅ Decision Tree Parameter Grid (no 'C')
param_grid = {
    'criterion': ['gini', 'entropy', 'log_loss'],
    'max_depth': [None, 3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'class_weight': [None, 'balanced']
}

grid = GridSearchCV(
    DecisionTreeClassifier(random_state=42),
    param_grid,
    refit=True,
    verbose=3,
    n_jobs=-1,
    scoring='f1_weighted'
)

# Fit the model
grid.fit(X,y)

# ✅ Logistic Regression Parameter Grid
# Logistic Regression with GridSearchCV
from sklearn.linear_model import LogisticRegression
param_grid = {
    'penalty': ['l1', 'l2', 'elasticnet', None],   # types of regularization
    'C': [0.01, 0.1, 1, 10, 100],                  # inverse of regularization strength
    'solver': ['lbfgs', 'liblinear', 'saga'],      # optimization solvers
    'class_weight': [None, 'balanced'],
    'max_iter': [100, 500, 1000]                   # number of iterations
}

# GridSearchCV
grid = GridSearchCV(
    LogisticRegression(random_state=42),
    param_grid,
    refit=True,
    verbose=3,
    n_jobs=-1,
    scoring='f1_weighted'
)

# Fit the model
grid.fit(X_train, y_train)

# Cross-validated predictions (instead of X_test)
y_pred = cross_val_predict(grid.best_estimator_, X, y, cv=5)

# Evaluation
from sklearn.metrics import confusion_matrix, classification_report, f1_score
cm = confusion_matrix(y, y_pred)
clf_report = classification_report(y, y_pred)
f1_macro = f1_score(y, y_pred, average='weighted')

print("Best Parameters:", grid.best_params_)
print("The f1_macro:", f1_macro)
print("The confusion Matrix:\n", cm)
print("The report:\n", clf_report)

# Save Best Model
import pickle
filename = "SVC_CV_best_model.sav"
pickle.dump(grid.best_estimator_, open(filename, 'wb'))

# CV Results table
results_df = pd.DataFrame(grid.cv_results_)
results_df.head()

# Load the saved model
loaded_model = pickle.load(open(filename, 'rb'))

# # Example of how to make predictions (replace with your new data)
new_data = pd.DataFrame([[30, 50000, 1]], columns=['Age', 'EstimatedSalary', 'Gender_Male']) # Example data
new_data_scaled = sc.transform(new_data) # Scale the new data
predictions = loaded_model.predict(new_data_scaled)
print("Predictions:", predictions)